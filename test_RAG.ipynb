{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install pymongo\n",
    "# !pip install langchain\n",
    "# !pip install langchain-community\n",
    "# !pip install pypdf\n",
    "# !pip install langchain-ollama\n",
    "# !pip install openai\n",
    "# !pip install tiktoken\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lzappate\\VSCodeProjects\\academy\\RAG_proj_notebook\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# ollama API\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# operate on LAB LLM\n",
    "# from langchain_community.chat_models import ChatOpenAI # DEPRECATED\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings # <<<<\n",
    "\n",
    "# langchain stuff\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.base import RunnableParallel\n",
    "\n",
    "# general\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "# Extra\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    ConfigurableFieldSpec,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import from project directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lzappate\\VSCodeProjects\\academy\\RAG_proj_notebook\\model\\chatbot\\withhistory.py:12: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from model.chatbot.memoryhistory import InMemoryHistory\n"
     ]
    }
   ],
   "source": [
    "from model import handlers\n",
    "from model.chatbot.withhistory import ChatbotWithHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Push to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Collection papers_collection_test_bis not found in database papers. Be aware that the collection is now being created.\n"
     ]
    }
   ],
   "source": [
    "handler = handlers.MongoHandler()\n",
    "handler.set_main_db('papers')\n",
    "handler.set_main_collection('papers_collection_test_bis')\n",
    "# handler\n",
    "# print(handler.papers.papers_collection_test)\n",
    "# print(handler.get_collections_from_database('papers'))\n",
    "# print(handler.get_collection('papers', 'papers_collection_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 16 documents loaded, with average characters equal to 4003.\n",
      "After split, there were 41 documents (chunks), with average characters equal to 1638 (average chunk length).\n"
     ]
    }
   ],
   "source": [
    "# load data, split and return dicts for storing them in mongo\n",
    "processor = handlers.PDFHandler(\"./files_pdf/\", verbose=True)\n",
    "dicts_after_split = processor.load_and_split(return_dicts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents pushed to collection 'papers_collection_test_bis' in database 'papers'\n"
     ]
    }
   ],
   "source": [
    "handler.push_to_main_collection(dicts_after_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create client and access DB/collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler created\n"
     ]
    }
   ],
   "source": [
    "if 'handler' not in globals():\n",
    "    handler = handlers.MongoHandler()\n",
    "    handler.set_main_db('papers')\n",
    "    handler.set_main_collection('papers_collection_test_bis')\n",
    "    print('handler created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = handler.retrieve_documents_from_main_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Embedding Model and VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lzappate\\VSCodeProjects\\academy\\RAG_proj_notebook\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceBgeEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",  # alternatively use \"sentence-transformers/all-MiniLM-l6-v2\" (faster)\n",
    "            model_kwargs={'device':'cpu'}, #CPU run or 'device': 'cuda' for GPU use\n",
    "            encode_kwargs={'normalize_embeddings': True} #Normalization is active, which means that the resulting vectors will have unit length. Normalization can be useful when you want to compare the similarity of sentences using methods like dot product or cosine similarity, as it makes the embeddings comparable on a common scale.\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'files_pdf\\\\1810.04805v2.pdf', 'page': 0}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016)'),\n",
       " Document(metadata={'source': 'files_pdf\\\\1810.04805v2.pdf', 'page': 2}, page_content='. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthetensor2tensor library.1Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERT BASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERT LARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERT BASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\\n4We note that in the literature the bidirectional Trans-'),\n",
       " Document(metadata={'source': 'files_pdf\\\\1810.04805v2.pdf', 'page': 3}, page_content='Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g.,⟨Question, Answer⟩) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ( [SEP] ). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence Aor sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token asC∈RH,\\nand the ﬁnal hidden vector for the ithinput token\\nasTi∈RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = handlers.VectorStoreHandler(embeddings_model=embeddings_model, documents=retrieved_docs[:20])\n",
    "\n",
    "vector_store.similarity_search(\"What is BERT?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.total_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.format_docs import format_docs\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate on LAB LLM\n",
    "# from langchain_community.chat_models import ChatOpenAI # DEPRECATED\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "# langchain stuff\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_base=\"https://047c-195-230-200-203.ngrok-free.app/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training procedure for BERT involves two steps:   1. Pre-training:\n",
      "The model is trained on unlabeled data over different pre-training tasks.\n",
      "2. Fine-tuning: The BERT model is first initialized with the pre-trained\n",
      "parameters, and all of the parameters are fine-tuned using labeled data\n",
      "from the downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"What is the pre-training procedure?\"\"\"\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=75)\n",
    "\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(wrapper.fill(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training procedure for BERT involves the following steps:\n",
      "\n",
      "1.  **Data Preparation**: \n",
      "    -   The input data is tokenized into subwords using WordPiece tokenization.\n",
      "    -   The input sequences are sampled to have a combined length of ≤512 tokens.\n",
      "    -   The LM masking is applied with a uniform masking rate of 15%.\n",
      "\n",
      "2.  **Training Setup**:\n",
      "    -   The model is trained with a batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch).\n",
      "    -   The training is done for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus.\n",
      "    -   The Adam optimizer is used with a learning rate of 1e-4, β1= 0.9, β2= 0.999, L2 weight decay of 0.01, and learning rate warmup over the first 10,000 steps.\n",
      "\n",
      "3.  **Training Procedure**:\n",
      "    -   The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.\n",
      "    -   The model is trained on 4 Cloud TPUs in Pod configuration (16 TPU chips total) for BERT BASE and 16 Cloud TPUs (64 TPU chips total) for BERT LARGE.\n",
      "    -   Each pre-training took 4 days to complete.\n",
      "\n",
      "4.  **Efficient Pre-training**:\n",
      "    -   To speed up pre-training, the model is pre-trained with sequence length of 128 for 90% of the steps.\n",
      "    -   Then, the model is trained with sequence length of 512 for the remaining 10% of the steps to learn the positional embeddings."
     ]
    }
   ],
   "source": [
    "# %%javascript\n",
    "# IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "#     return false;\n",
    "# }\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# wrapper = textwrap.TextWrapper(width=75)\n",
    "\n",
    "response = ''\n",
    "question = \"\"\"What is the pre-training procedure? Please wrap the text width to 80 characters.\"\"\"\n",
    "for chunk in chain.stream(question):\n",
    "    response+=chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# clear_output(wait=True)\n",
    "\n",
    "# print(wrapper.fill(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domanda</th>\n",
       "      <th>Risposta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does BERT stand for?</td>\n",
       "      <td>BERT stands for Bidirectional Encoder Represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the main innovation of BERT compared t...</td>\n",
       "      <td>BERT’s main innovation is its ability to pre-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does BERT differ from traditional language...</td>\n",
       "      <td>Traditional language models are unidirectional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some tasks BERT achieves state-of-the...</td>\n",
       "      <td>BERT achieves state-of-the-art results on task...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of the [MASK] token i...</td>\n",
       "      <td>The [MASK] token is used during pre-training t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Domanda  \\\n",
       "0                          What does BERT stand for?   \n",
       "1  What is the main innovation of BERT compared t...   \n",
       "2  How does BERT differ from traditional language...   \n",
       "3  What are some tasks BERT achieves state-of-the...   \n",
       "4  What is the significance of the [MASK] token i...   \n",
       "\n",
       "                                            Risposta  \n",
       "0  BERT stands for Bidirectional Encoder Represen...  \n",
       "1  BERT’s main innovation is its ability to pre-t...  \n",
       "2  Traditional language models are unidirectional...  \n",
       "3  BERT achieves state-of-the-art results on task...  \n",
       "4  The [MASK] token is used during pre-training t...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_df = pd.read_excel(\"TestSetBERT.xlsx\")\n",
    "testset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domanda</th>\n",
       "      <th>Risposta</th>\n",
       "      <th>Model Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does BERT stand for?</td>\n",
       "      <td>BERT stands for Bidirectional Encoder Represen...</td>\n",
       "      <td>BERT stands for Bidirectional Encoder Represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the main innovation of BERT compared t...</td>\n",
       "      <td>BERT’s main innovation is its ability to pre-t...</td>\n",
       "      <td>The main innovation of BERT compared to previo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does BERT differ from traditional language...</td>\n",
       "      <td>Traditional language models are unidirectional...</td>\n",
       "      <td>BERT differs from traditional language models,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some tasks BERT achieves state-of-the...</td>\n",
       "      <td>BERT achieves state-of-the-art results on task...</td>\n",
       "      <td>BERT achieves state-of-the-art results on elev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of the [MASK] token i...</td>\n",
       "      <td>The [MASK] token is used during pre-training t...</td>\n",
       "      <td>The [MASK] token in BERT is used for the Maske...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Domanda  \\\n",
       "0                          What does BERT stand for?   \n",
       "1  What is the main innovation of BERT compared t...   \n",
       "2  How does BERT differ from traditional language...   \n",
       "3  What are some tasks BERT achieves state-of-the...   \n",
       "4  What is the significance of the [MASK] token i...   \n",
       "\n",
       "                                            Risposta  \\\n",
       "0  BERT stands for Bidirectional Encoder Represen...   \n",
       "1  BERT’s main innovation is its ability to pre-t...   \n",
       "2  Traditional language models are unidirectional...   \n",
       "3  BERT achieves state-of-the-art results on task...   \n",
       "4  The [MASK] token is used during pre-training t...   \n",
       "\n",
       "                                      Model Response  \n",
       "0  BERT stands for Bidirectional Encoder Represen...  \n",
       "1  The main innovation of BERT compared to previo...  \n",
       "2  BERT differs from traditional language models,...  \n",
       "3  BERT achieves state-of-the-art results on elev...  \n",
       "4  The [MASK] token in BERT is used for the Maske...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_responses = []\n",
    "for i in range(testset_df.shape[0]):\n",
    "    response = chain.invoke(testset_df.iloc[i, 0])\n",
    "    model_responses.append(response)\n",
    "testset_df[\"Model Response\"] = model_responses\n",
    "testset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.to_excel(\"TestSetBERT_with_model_responses.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: RunnableWithMessageHistory\n",
    "\n",
    "[Discussion on GitHub](https://github.com/langchain-ai/langchain/discussions/16582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "chatbot = ChatbotWithHistory(user_id=\"zappa\", conversation_id=\"dev\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no previous interaction history to draw from. The conversation has just started.\n",
      "\n",
      "\n",
      "You asked me \"Why is the sky blue?\" in the last question.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Why is the sky blue?\"\n",
    "response = chatbot.ask_question(question)\n",
    "\n",
    "question = \"what did I asked you in the last question?\"\n",
    "response = chatbot.ask_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training procedure for BERT involves two steps: pre-training and fine-tuning. \n",
      "\n",
      "During pre-training, the model is trained on unlabeled data over different pre-training tasks. The pre-training tasks used in BERT are:\n",
      "\n",
      "1. Masked Language Model (MLM): This task involves randomly masking some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked token.\n",
      "2. Next Sentence Prediction (NSP): This task involves predicting whether two sentences are adjacent in the original text or not.\n",
      "\n",
      "The pre-training procedure for BERT is as follows:\n",
      "\n",
      "1. The model is initialized with random weights.\n",
      "2. The model is trained on a large corpus of text data.\n",
      "3. During training, some of the tokens in the input are randomly masked.\n",
      "4. The model predicts the original vocabulary id of the masked token.\n",
      "5. The model is also trained to predict whether two sentences are adjacent in the original text or not.\n",
      "6. The model is trained to minimize the loss on both tasks.\n",
      "\n",
      "The pre-trained model is then fine-tuned on labeled data from downstream tasks. During fine-tuning, all parameters of the pre-trained model are fine-tuned. The fine-tuning procedure involves:\n",
      "\n",
      "1. The pre-trained model is initialized with the pre-trained weights.\n",
      "2. The model is trained on labeled data from the downstream task.\n",
      "3. All parameters of the model are fine-tuned to minimize the loss on the downstream task.\n",
      "\n",
      "The pre-trained model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference.\n",
      "\n",
      "\n",
      "You asked me the following questions during this conversation:\n",
      "\n",
      "1. Why is the sky blue?\n",
      "2. What did I ask you in the last question?\n",
      "3. What is the pre-training procedure?\n",
      "4. Which questions did I ask during this conversation?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the pre-training procedure?\"\n",
    "response = chatbot.ask_question(question)\n",
    "\n",
    "# question = \"And What is BERT?\"\n",
    "# response = chatbot.ask_question(question)\n",
    "\n",
    "question = \"Which questions did I ask during this conversation?\"\n",
    "response = chatbot.ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exampe of history:\n",
    "\n",
    "- Code:\n",
    "```python\n",
    "print(store.get(('zappa', 'foo')))\n",
    "```\n",
    "\n",
    "- Output:\n",
    "\n",
    "Human: What is the pre-training procedure?\n",
    "AI: The pre-training procedure for BERT involves two steps: \n",
    "\n",
    "1. Pre-training: The model is trained on unlabeled data over different pre-training tasks. \n",
    "2. Fine-tuning: The BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. \n",
    "\n",
    "During pre-training, the model uses a \"masked language model\" (MLM) pre-training objective, where some of the tokens from the input are randomly masked, and the objective is to predict the original vocabulary id of the masked tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Why is the sky blue?\n",
      "AI: There is no previous interaction history to draw from. The conversation has just started.\n",
      "Human: what did I asked you in the last question?\n",
      "AI: You asked me \"Why is the sky blue?\" in the last question.\n",
      "Human: What is the pre-training procedure?\n",
      "AI: The pre-training procedure for BERT involves two steps: pre-training and fine-tuning. \n",
      "\n",
      "During pre-training, the model is trained on unlabeled data over different pre-training tasks. The pre-training tasks used in BERT are:\n",
      "\n",
      "1. Masked Language Model (MLM): This task involves randomly masking some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked token.\n",
      "2. Next Sentence Prediction (NSP): This task involves predicting whether two sentences are adjacent in the original text or not.\n",
      "\n",
      "The pre-training procedure for BERT is as follows:\n",
      "\n",
      "1. The model is initialized with random weights.\n",
      "2. The model is trained on a large corpus of text data.\n",
      "3. During training, some of the tokens in the input are randomly masked.\n",
      "4. The model predicts the original vocabulary id of the masked token.\n",
      "5. The model is also trained to predict whether two sentences are adjacent in the original text or not.\n",
      "6. The model is trained to minimize the loss on both tasks.\n",
      "\n",
      "The pre-trained model is then fine-tuned on labeled data from downstream tasks. During fine-tuning, all parameters of the pre-trained model are fine-tuned. The fine-tuning procedure involves:\n",
      "\n",
      "1. The pre-trained model is initialized with the pre-trained weights.\n",
      "2. The model is trained on labeled data from the downstream task.\n",
      "3. All parameters of the model are fine-tuned to minimize the loss on the downstream task.\n",
      "\n",
      "The pre-trained model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference.\n",
      "Human: Which questions did I ask during this conversation?\n",
      "AI: You asked me the following questions during this conversation:\n",
      "\n",
      "1. Why is the sky blue?\n",
      "2. What did I ask you in the last question?\n",
      "3. What is the pre-training procedure?\n",
      "4. Which questions did I ask during this conversation?\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.history.get(('zappa', 'dev')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and I think of the absurdity of the situation. A penguin, a flightless bird, holding a rifle. It's a comical image, and I can almost hear the penguin's awkward attempts to hold the rifle steady.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "o = OpenAI(\n",
    "    openai_api_base=\"https://c12d-195-230-200-203.ngrok-free.app/v1\",\n",
    "    api_key=\"EMPTY\", max_tokens=50, temperature=0\n",
    "    )\n",
    "\n",
    "response = o.invoke(\"I see a penguin with a rifle\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
