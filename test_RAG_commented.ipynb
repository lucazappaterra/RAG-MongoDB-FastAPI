{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install pymongo\n",
    "# !pip install langchain\n",
    "# !pip install langchain-community\n",
    "# !pip install pypdf\n",
    "# !pip install langchain-ollama\n",
    "# !pip install openai\n",
    "# !pip install tiktoken\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongo-related\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "# pdf loading/splitting\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "# ollama API\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# operate on LAB LLM\n",
    "# from langchain_community.chat_models import ChatOpenAI # DEPRECATED\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# langchain stuff\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.base import RunnableParallel\n",
    "\n",
    "# general\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "# Extra\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    ConfigurableFieldSpec,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes definition test (not implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoHandler(MongoClient):\n",
    "    def __init__(self, host='localhost', port=27017):\n",
    "        super().__init__(host, port)\n",
    "        self.dbs = {self[key] for key in self.list_database_names()}\n",
    "\n",
    "    def get_collections_from_database(self, db_name):\n",
    "        db = self[db_name]\n",
    "        return db.list_collection_names()\n",
    "    \n",
    "    def get_collection(self, db_name, collection_name):\n",
    "        db = self[db_name]\n",
    "        if collection_name in db.list_collection_names():\n",
    "            return db[collection_name]  \n",
    "        else: \n",
    "            raise ValueError(f\"Collection {collection_name} not found in database {db_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = MongoHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoHandler(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'papers'), 'papers_collection_test')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.papers.papers_collection_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['papers_collection_test']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.get_collections_from_database('papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoHandler(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'papers'), 'papers_collection_test')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.get_collection('papers', 'papers_collection_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'files_pdf\\\\1810.04805v2.pdf', 'page': 0}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016)')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./files_pdf/\")\n",
    "\n",
    "docs_before_split = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\".\", \"\\n\\n\"],\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "docs_after_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 16 documents loaded, with average characters equal to 4003.\n",
      "After split, there were 41 documents (chunks), with average characters equal to 1638 (average chunk length).\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local', 'papers', 'test_db']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.papers\n",
    "collection = db.papers_collection_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_after_split = [chunk.model_dump() for chunk in docs_after_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! per completezza si poteva inserire una funzione di \"insert\" nell'handler di Mongo definito sopra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('6717b904ee55f62c083a22d8'), ObjectId('6717b904ee55f62c083a22d9'), ObjectId('6717b904ee55f62c083a22da'), ObjectId('6717b904ee55f62c083a22db'), ObjectId('6717b904ee55f62c083a22dc'), ObjectId('6717b904ee55f62c083a22dd'), ObjectId('6717b904ee55f62c083a22de'), ObjectId('6717b904ee55f62c083a22df'), ObjectId('6717b904ee55f62c083a22e0'), ObjectId('6717b904ee55f62c083a22e1'), ObjectId('6717b904ee55f62c083a22e2'), ObjectId('6717b904ee55f62c083a22e3'), ObjectId('6717b904ee55f62c083a22e4'), ObjectId('6717b904ee55f62c083a22e5'), ObjectId('6717b904ee55f62c083a22e6'), ObjectId('6717b904ee55f62c083a22e7'), ObjectId('6717b904ee55f62c083a22e8'), ObjectId('6717b904ee55f62c083a22e9'), ObjectId('6717b904ee55f62c083a22ea'), ObjectId('6717b904ee55f62c083a22eb'), ObjectId('6717b904ee55f62c083a22ec'), ObjectId('6717b904ee55f62c083a22ed'), ObjectId('6717b904ee55f62c083a22ee'), ObjectId('6717b904ee55f62c083a22ef'), ObjectId('6717b904ee55f62c083a22f0'), ObjectId('6717b904ee55f62c083a22f1'), ObjectId('6717b904ee55f62c083a22f2'), ObjectId('6717b904ee55f62c083a22f3'), ObjectId('6717b904ee55f62c083a22f4'), ObjectId('6717b904ee55f62c083a22f5'), ObjectId('6717b904ee55f62c083a22f6'), ObjectId('6717b904ee55f62c083a22f7'), ObjectId('6717b904ee55f62c083a22f8'), ObjectId('6717b904ee55f62c083a22f9'), ObjectId('6717b904ee55f62c083a22fa'), ObjectId('6717b904ee55f62c083a22fb'), ObjectId('6717b904ee55f62c083a22fc'), ObjectId('6717b904ee55f62c083a22fd'), ObjectId('6717b904ee55f62c083a22fe'), ObjectId('6717b904ee55f62c083a22ff'), ObjectId('6717b904ee55f62c083a2300')], acknowledged=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert_many(dicts_after_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create client and access DB/collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'client' not in globals():\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.papers\n",
    "    collection = db.papers_collection_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! si poteva inserire in una funzione dell'handler di Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Document'\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "for chunk in collection.find():\n",
    "    pprint.pprint(chunk.get(\"type\"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents' chunks reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = []\n",
    "for chunk in collection.find():\n",
    "    doc = Document(page_content=chunk.get(\"page_content\"), \n",
    "                   metadata=chunk.get(\"metadata\"),\n",
    "                   type='Document',\n",
    "                   id=chunk.get(\"_id\"))\n",
    "    retrieved_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lzappate\\VSCodeProjects\\academy\\RAG_proj\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # alternatively use \"sentence-transformers/all-MiniLM-l6-v2\" (faster)\n",
    "    model_kwargs={'device':'cpu'}, #CPU run or 'device': 'cuda' for GPU use\n",
    "    encode_kwargs={'normalize_embeddings': True} #Normalization is active, which means that the resulting vectors will have unit length. Normalization can be useful when you want to compare the similarity of sentences using methods like dot product or cosine similarity, as it makes the embeddings comparable on a common scale.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "sample_embedding = np.array(huggingface_embeddings.embed_query(retrieved_docs[0].page_content))\n",
    "print(sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(retrieved_docs, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! si poteva creare una funzione invece di replicare il codice più volte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 documents retrieved which are relevant to the query. Display the first one:\n",
      "\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for\n",
      "Language Understanding\n",
      "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "Google AI Language\n",
      "{jacobdevlin,mingweichang,kentonl,kristout }@google.com\n",
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT , which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be ﬁne-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "speciﬁc architecture modiﬁcations.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute improvement),\n",
      "MultiNLI accuracy to 86.7% (4.6% absolute\n",
      "improvement), SQuAD v1.1 question answer-\n",
      "ing Test F1 to 93.2 (1.5 point absolute im-\n",
      "provement) and SQuAD v2.0 Test F1 to 83.1\n",
      "(5.1 point absolute improvement).\n",
      "1 Introduction\n",
      "Language model pre-training has been shown to\n",
      "be effective for improving many natural language\n",
      "processing tasks (Dai and Le, 2015; Peters et al.,\n",
      "2018a; Radford et al., 2018; Howard and Ruder,\n",
      "2018). These include sentence-level tasks such as\n",
      "natural language inference (Bowman et al., 2015;\n",
      "Williams et al., 2018) and paraphrasing (Dolan\n",
      "and Brockett, 2005), which aim to predict the re-\n",
      "lationships between sentences by analyzing them\n",
      "holistically, as well as token-level tasks such as\n",
      "named entity recognition and question answering,\n",
      "where models are required to produce ﬁne-grained\n",
      "output at the token level (Tjong Kim Sang and\n",
      "De Meulder, 2003; Rajpurkar et al., 2016)\n",
      "\n",
      "Display the second one:\n",
      ". For ﬁne-\n",
      "tuning, the BERT model is ﬁrst initialized with\n",
      "the pre-trained parameters, and all of the param-\n",
      "eters are ﬁne-tuned using labeled data from the\n",
      "downstream tasks. Each downstream task has sep-\n",
      "arate ﬁne-tuned models, even though they are ini-\n",
      "tialized with the same pre-trained parameters. The\n",
      "question-answering example in Figure 1 will serve\n",
      "as a running example for this section.\n",
      "A distinctive feature of BERT is its uniﬁed ar-\n",
      "chitecture across different tasks. There is mini-mal difference between the pre-trained architec-\n",
      "ture and the ﬁnal downstream architecture.\n",
      "Model Architecture BERT’s model architec-\n",
      "ture is a multi-layer bidirectional Transformer en-\n",
      "coder based on the original implementation de-\n",
      "scribed in Vaswani et al. (2017) and released in\n",
      "thetensor2tensor library.1Because the use\n",
      "of Transformers has become common and our im-\n",
      "plementation is almost identical to the original,\n",
      "we will omit an exhaustive background descrip-\n",
      "tion of the model architecture and refer readers to\n",
      "Vaswani et al. (2017) as well as excellent guides\n",
      "such as “The Annotated Transformer.”2\n",
      "In this work, we denote the number of layers\n",
      "(i.e., Transformer blocks) as L, the hidden size as\n",
      "H, and the number of self-attention heads as A.3\n",
      "We primarily report results on two model sizes:\n",
      "BERT BASE (L=12, H=768, A=12, Total Param-\n",
      "eters=110M) and BERT LARGE (L=24, H=1024,\n",
      "A=16, Total Parameters=340M).\n",
      "BERT BASE was chosen to have the same model\n",
      "size as OpenAI GPT for comparison purposes.\n",
      "Critically, however, the BERT Transformer uses\n",
      "bidirectional self-attention, while the GPT Trans-\n",
      "former uses constrained self-attention where every\n",
      "token can only attend to context to its left.4\n",
      "1https://github.com/tensorﬂow/tensor2tensor\n",
      "2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
      "3In all cases we set the feed-forward/ﬁlter size to be 4H,\n",
      "i.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n",
      "4We note that in the literature the bidirectional Trans-\n",
      "\n",
      "Display the third one:\n",
      ". For the largest and most widely\n",
      "reported GLUE task, MNLI, BERT obtains a 4.6%\n",
      "absolute accuracy improvement. On the ofﬁcial\n",
      "GLUE leaderboard10, BERT LARGE obtains a score\n",
      "of 80.5, compared to OpenAI GPT, which obtains\n",
      "72.8 as of the date of writing.\n",
      "We ﬁnd that BERT LARGE signiﬁcantly outper-\n",
      "forms BERT BASE across all tasks, especially those\n",
      "with very little training data. The effect of model\n",
      "size is explored more thoroughly in Section 5.2.\n",
      "4.2 SQuAD v1.1\n",
      "The Stanford Question Answering Dataset\n",
      "(SQuAD v1.1) is a collection of 100k crowd-\n",
      "sourced question/answer pairs (Rajpurkar et al.,\n",
      "2016). Given a question and a passage from\n",
      "9The GLUE data set distribution does not include the Test\n",
      "labels, and we only made a single GLUE evaluation server\n",
      "submission for each of BERT BASE and BERT LARGE .\n",
      "10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\n",
      "predict the answer text span in the passage.\n",
      "As shown in Figure 1, in the question answer-\n",
      "ing task, we represent the input question and pas-\n",
      "sage as a single packed sequence, with the ques-\n",
      "tion using the Aembedding and the passage using\n",
      "theBembedding. We only introduce a start vec-\n",
      "torS∈RHand an end vector E∈RHduring\n",
      "ﬁne-tuning. The probability of word ibeing the\n",
      "start of the answer span is computed as a dot prod-\n",
      "uct between TiandSfollowed by a softmax over\n",
      "all of the words in the paragraph: Pi=eS·Ti∑\n",
      "jeS·Tj.\n",
      "The analogous formula is used for the end of the\n",
      "answer span. The score of a candidate span from\n",
      "positionito positionjis deﬁned as S·Ti+E·Tj,\n",
      "and the maximum scoring span where j≥iis\n",
      "used as a prediction. The training objective is the\n",
      "sum of the log-likelihoods of the correct start and\n",
      "end positions. We ﬁne-tune for 3 epochs with a\n",
      "learning rate of 5e-5 and a batch size of 32.\n",
      "Table 2 shows top leaderboard entries as well\n",
      "as results from top published systems (Seo et al.,\n",
      "2017; Clark and Gardner, 2018; Peters et al.,\n",
      "2018a; Hu et al., 2018)\n",
      "\n",
      "Display the fourth one:\n",
      "System Dev Test\n",
      "EM F1 EM F1\n",
      "Top Leaderboard Systems (Dec 10th, 2018)\n",
      "Human - - 82.3 91.2\n",
      "#1 Ensemble - nlnet - - 86.0 91.7\n",
      "#2 Ensemble - QANet - - 84.5 90.5\n",
      "Published\n",
      "BiDAF+ELMo (Single) - 85.6 - 85.8\n",
      "R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\n",
      "Ours\n",
      "BERT BASE (Single) 80.8 88.5 - -\n",
      "BERT LARGE (Single) 84.1 90.9 - -\n",
      "BERT LARGE (Ensemble) 85.8 91.8 - -\n",
      "BERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\n",
      "BERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\n",
      "Table 2: SQuAD 1.1 results. The BERT ensemble\n",
      "is 7x systems which use different pre-training check-\n",
      "points and ﬁne-tuning seeds.\n",
      "System Dev Test\n",
      "EM F1 EM F1\n",
      "Top Leaderboard Systems (Dec 10th, 2018)\n",
      "Human 86.3 89.0 86.9 89.5\n",
      "#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\n",
      "#2 Single - nlnet - - 74.2 77.1\n",
      "Published\n",
      "unet (Ensemble) - - 71.4 74.9\n",
      "SLQA+ (Single) - 71.4 74.4\n",
      "Ours\n",
      "BERT LARGE (Single) 78.7 81.9 80.0 83.1\n",
      "Table 3: SQuAD 2.0 results. We exclude entries that\n",
      "use BERT as one of their components.\n",
      "tuning data, we only lose 0.1-0.4 F1, still outper-\n",
      "forming all existing systems by a wide margin.12\n",
      "4.3 SQuAD v2.0\n",
      "The SQuAD 2.0 task extends the SQuAD 1.1\n",
      "problem deﬁnition by allowing for the possibility\n",
      "that no short answer exists in the provided para-\n",
      "graph, making the problem more realistic.\n",
      "We use a simple approach to extend the SQuAD\n",
      "v1.1 BERT model for this task. We treat ques-\n",
      "tions that do not have an answer as having an an-\n",
      "swer span with start and end at the [CLS] to-\n",
      "ken. The probability space for the start and end\n",
      "answer span positions is extended to include the\n",
      "position of the [CLS] token. For prediction, we\n",
      "compare the score of the no-answer span: snull=\n",
      "S·C+E·Cto the score of the best non-null span\n",
      "12The TriviaQA data we used consists of paragraphs from\n",
      "TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\n",
      "that contain at least one of the provided possible answers.System Dev Test\n",
      "ESIM+GloVe 51.9 52.7\n",
      "ESIM+ELMo 59.1 59.2\n",
      "OpenAI GPT - 78.0\n",
      "BERT BASE 81.6 -\n",
      "BERT LARGE 86.6 86.3\n",
      "Human (expert)†- 85\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Test the similarity search\n",
    "query = \"\"\"What does 'BERT' stand for?\"\"\"\n",
    "relevant_documents = vectorstore.similarity_search(query, k=8)\n",
    "\n",
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)\n",
    "print(\"\\nDisplay the second one:\")\n",
    "print(relevant_documents[1].page_content)\n",
    "print(\"\\nDisplay the third one:\")\n",
    "print(relevant_documents[2].page_content)\n",
    "print(\"\\nDisplay the fourth one:\")\n",
    "print(relevant_documents[3].page_content)\n",
    "print(type(relevant_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_base=\"https://d0b9-195-230-200-203.ngrok-free.app/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training procedure for BERT involves two steps:   1. Pre-training:\n",
      "The model is trained on unlabeled data over different pre-training tasks.\n",
      "2. Fine-tuning: The BERT model is first initialized with the pre-trained\n",
      "parameters, and all of the parameters are fine-tuned using labeled data\n",
      "from the downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"What is the pre-training procedure?\"\"\"\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=75)\n",
    "\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(wrapper.fill(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training procedure for BERT involves the following steps:\n",
      "\n",
      "1.  **Data Preparation**: \n",
      "    -   The input data is tokenized into subwords using WordPiece tokenization.\n",
      "    -   The input sequences are sampled to have a combined length of ≤512 tokens.\n",
      "    -   The LM masking is applied with a uniform masking rate of 15%.\n",
      "\n",
      "2.  **Training Setup**:\n",
      "    -   The model is trained with a batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch).\n",
      "    -   The training is done for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus.\n",
      "    -   The Adam optimizer is used with a learning rate of 1e-4, β1= 0.9, β2= 0.999, L2 weight decay of 0.01, and learning rate warmup over the first 10,000 steps.\n",
      "\n",
      "3.  **Training Procedure**:\n",
      "    -   The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.\n",
      "    -   The model is trained on 4 Cloud TPUs in Pod configuration (16 TPU chips total) for BERT BASE and 16 Cloud TPUs (64 TPU chips total) for BERT LARGE.\n",
      "    -   Each pre-training took 4 days to complete.\n",
      "\n",
      "4.  **Efficient Pre-training**:\n",
      "    -   To speed up pre-training, the model is pre-trained with sequence length of 128 for 90% of the steps.\n",
      "    -   Then, the model is trained with sequence length of 512 for the remaining 10% of the steps to learn the positional embeddings."
     ]
    }
   ],
   "source": [
    "# %%javascript\n",
    "# IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "#     return false;\n",
    "# }\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# wrapper = textwrap.TextWrapper(width=75)\n",
    "\n",
    "response = ''\n",
    "question = \"\"\"What is the pre-training procedure? Please wrap the text width to 80 characters.\"\"\"\n",
    "for chunk in chain.stream(question):\n",
    "    response+=chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# clear_output(wait=True)\n",
    "\n",
    "# print(wrapper.fill(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domanda</th>\n",
       "      <th>Risposta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does BERT stand for?</td>\n",
       "      <td>BERT stands for Bidirectional Encoder Represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the main innovation of BERT compared t...</td>\n",
       "      <td>BERT’s main innovation is its ability to pre-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does BERT differ from traditional language...</td>\n",
       "      <td>Traditional language models are unidirectional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some tasks BERT achieves state-of-the...</td>\n",
       "      <td>BERT achieves state-of-the-art results on task...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of the [MASK] token i...</td>\n",
       "      <td>The [MASK] token is used during pre-training t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Domanda  \\\n",
       "0                          What does BERT stand for?   \n",
       "1  What is the main innovation of BERT compared t...   \n",
       "2  How does BERT differ from traditional language...   \n",
       "3  What are some tasks BERT achieves state-of-the...   \n",
       "4  What is the significance of the [MASK] token i...   \n",
       "\n",
       "                                            Risposta  \n",
       "0  BERT stands for Bidirectional Encoder Represen...  \n",
       "1  BERT’s main innovation is its ability to pre-t...  \n",
       "2  Traditional language models are unidirectional...  \n",
       "3  BERT achieves state-of-the-art results on task...  \n",
       "4  The [MASK] token is used during pre-training t...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_df = pd.read_excel(\"TestSetBERT.xlsx\")\n",
    "testset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domanda</th>\n",
       "      <th>Risposta</th>\n",
       "      <th>Model Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does BERT stand for?</td>\n",
       "      <td>BERT stands for Bidirectional Encoder Represen...</td>\n",
       "      <td>BERT stands for Bidirectional Encoder Represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the main innovation of BERT compared t...</td>\n",
       "      <td>BERT’s main innovation is its ability to pre-t...</td>\n",
       "      <td>The main innovation of BERT compared to previo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does BERT differ from traditional language...</td>\n",
       "      <td>Traditional language models are unidirectional...</td>\n",
       "      <td>BERT differs from traditional language models,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some tasks BERT achieves state-of-the...</td>\n",
       "      <td>BERT achieves state-of-the-art results on task...</td>\n",
       "      <td>BERT achieves state-of-the-art results on elev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of the [MASK] token i...</td>\n",
       "      <td>The [MASK] token is used during pre-training t...</td>\n",
       "      <td>The [MASK] token in BERT is used for the Maske...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Domanda  \\\n",
       "0                          What does BERT stand for?   \n",
       "1  What is the main innovation of BERT compared t...   \n",
       "2  How does BERT differ from traditional language...   \n",
       "3  What are some tasks BERT achieves state-of-the...   \n",
       "4  What is the significance of the [MASK] token i...   \n",
       "\n",
       "                                            Risposta  \\\n",
       "0  BERT stands for Bidirectional Encoder Represen...   \n",
       "1  BERT’s main innovation is its ability to pre-t...   \n",
       "2  Traditional language models are unidirectional...   \n",
       "3  BERT achieves state-of-the-art results on task...   \n",
       "4  The [MASK] token is used during pre-training t...   \n",
       "\n",
       "                                      Model Response  \n",
       "0  BERT stands for Bidirectional Encoder Represen...  \n",
       "1  The main innovation of BERT compared to previo...  \n",
       "2  BERT differs from traditional language models,...  \n",
       "3  BERT achieves state-of-the-art results on elev...  \n",
       "4  The [MASK] token in BERT is used for the Maske...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_responses = []\n",
    "for i in range(testset_df.shape[0]):\n",
    "    response = chain.invoke(testset_df.iloc[i, 0])\n",
    "    model_responses.append(response)\n",
    "testset_df[\"Model Response\"] = model_responses\n",
    "testset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.to_excel(\"TestSetBERT_with_model_responses.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: RunnableWithMessageHistory\n",
    "\n",
    "[Discussion on GitHub](https://github.com/langchain-ai/langchain/discussions/16582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        \"\"\"Add a self-created message to the store\"\"\"\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "# Here we use a global variable to store the chat message history.\n",
    "# This will make it easier to inspect it to see the underlying results.\n",
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = InMemoryHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "## Test the history\n",
    "# history = get_session_history(\"1\", \"1\")\n",
    "# history.add_message(AIMessage(content=\"hello\"))\n",
    "# print(store) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    If asked for previous interactions, you need to answer based on the following history:\n",
    "    \n",
    "    {history}\n",
    "    \n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_base=\"https://d0b9-195-230-200-203.ngrok-free.app/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# retrieve_context_chain = itemgetter(\"question\") | retriever | format_docs\n",
    "# retrieve_history_chain = itemgetter(\"history\")\n",
    "\n",
    "# first_step = RunnablePassthrough.assign(context=retrieve_context_chain, history=retrieve_history_chain)\n",
    "# chain = first_step | prompt | model #| StrOutputParser()\n",
    "# chain = {\"response\": first_step | prompt | model, \"context\": itemgetter(\"context\")} #| StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel({\n",
    "                      \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
    "                      \"question\": itemgetter(\"question\"),\n",
    "                      \"history\": itemgetter(\"history\")\n",
    "    })\n",
    "    |{\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"output\": prompt | model,\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "        \"history\": itemgetter(\"history\")\n",
    "    }\n",
    "    # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    # Uses the get_by_session_id function defined before\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! si può migliorare l'utilizzo dei parametri user_id e conversation_id passandoli come parametri della funzione o inserendo il tutto in una classe e definirli nell costruttore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! manca salvataggio delle conversazioni su mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training procedure for BERT involves two steps: pre-training and fine-tuning. \n",
      "\n",
      "During pre-training, the model is trained on unlabeled data over different pre-training tasks. \n",
      "\n",
      "For pre-training, the model uses a \"masked language model\" (MLM) objective, where some of the tokens from the input are randomly masked, and the objective is to predict the original vocabulary id of the masked tokens. \n",
      "\n",
      "Additionally, BERT also uses a \"next sentence prediction\" task that jointly pre-trains text-pair representations. \n",
      "\n",
      "The pre-training procedure is shown in Figure 1, which illustrates the overall pre-training and fine-tuning procedures for BERT.\n",
      "\n",
      "\n",
      "BERT stands for Bidirectional Encoder Representations from Transformers. It is a new language representation model introduced in the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
      "\n",
      "\n",
      "You asked the following questions during this conversation:\n",
      "\n",
      "1. What is the pre-training procedure?\n",
      "2. And What is BERT?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"question\": question},\n",
    "        config={\"configurable\": {\"user_id\": user_id, \"conversation_id\": conversation_id}}\n",
    "    )\n",
    "    print(response.get(\"output\").content)\n",
    "    print(\"\\n\")\n",
    "    return response\n",
    "\n",
    "\n",
    "user_id = \"zappa\"\n",
    "conversation_id = \"dev\"\n",
    "\n",
    "\n",
    "question = \"What is the pre-training procedure?\"\n",
    "response = ask_question(question)\n",
    "\n",
    "question = \"And What is BERT?\"\n",
    "response = ask_question(question)\n",
    "\n",
    "question = \"Which questions did I ask during this conversation?\"\n",
    "response = ask_question(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exampe of history:\n",
    "\n",
    "- Code:\n",
    "```python\n",
    "print(store.get(('zappa', 'foo')))\n",
    "```\n",
    "\n",
    "- Output:\n",
    "\n",
    "Human: What is the pre-training procedure?\n",
    "AI: The pre-training procedure for BERT involves two steps: \n",
    "\n",
    "1. Pre-training: The model is trained on unlabeled data over different pre-training tasks. \n",
    "2. Fine-tuning: The BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. \n",
    "\n",
    "During pre-training, the model uses a \"masked language model\" (MLM) pre-training objective, where some of the tokens from the input are randomly masked, and the objective is to predict the original vocabulary id of the masked tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What does cosine mean?\n",
      "AI: The cosine is a fundamental concept in mathematics, particularly in trigonometry. It's a ratio of the adjacent side to the hypotenuse in a right-angled triangle.\n",
      "\n",
      "Imagine a right-angled triangle with an angle θ (theta) between the two sides. The cosine of θ (cos θ) is defined as the ratio of the length of the adjacent side (the side next to the angle) to the length of the hypotenuse (the side opposite the right angle).\n",
      "\n",
      "Mathematically, it's represented as:\n",
      "\n",
      "cos θ = adjacent side / hypotenuse\n",
      "\n",
      "For example, if the adjacent side is 3 units and the hypotenuse is 5 units, the cosine of θ would be:\n",
      "\n",
      "cos θ = 3 / 5 = 0.6\n",
      "\n",
      "The cosine function is used to describe the ratio of the adjacent side to the hypotenuse in a right-angled triangle, and it's a fundamental concept in mathematics, physics, engineering, and many other fields.\n",
      "\n",
      "Would you like to know more about trigonometry or cosine?\n",
      "Human: What's its inverse\n",
      "AI: The inverse of the cosine function is called the arccosine (or cos^-1). It's a mathematical operation that finds the angle whose cosine is a given value.\n",
      "\n",
      "In other words, if you know the cosine of an angle, the arccosine function will give you the angle itself.\n",
      "\n",
      "Mathematically, it's represented as:\n",
      "\n",
      "θ = arccos(x)\n",
      "\n",
      "where x is the cosine of the angle θ.\n",
      "\n",
      "For example, if you know that the cosine of an angle is 0.6, the arccosine function will give you the angle itself:\n",
      "\n",
      "θ = arccos(0.6)\n",
      "\n",
      "Using a calculator or a mathematical library, we can find that:\n",
      "\n",
      "θ ≈ 53.13°\n",
      "\n",
      "So, the arccosine of 0.6 is approximately 53.13°.\n",
      "\n",
      "The arccosine function is used in many mathematical and scientific applications, such as solving right-angled triangles, finding angles in physics and engineering, and more.\n",
      "\n",
      "Here's a simple formula to remember:\n",
      "\n",
      "cos θ = adjacent side / hypotenuse\n",
      "θ = arccos(adjacent side / hypotenuse)\n",
      "\n",
      "Does that make sense?\n"
     ]
    }
   ],
   "source": [
    "print(store.get('foo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and I think of the absurdity of the situation. A penguin, a flightless bird, holding a rifle. It's a comical image, and I can almost hear the penguin's awkward attempts to hold the rifle steady.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "o = OpenAI(\n",
    "    openai_api_base=\"https://c12d-195-230-200-203.ngrok-free.app/v1\",\n",
    "    api_key=\"EMPTY\", max_tokens=50, temperature=0\n",
    "    )\n",
    "\n",
    "response = o.invoke(\"I see a penguin with a rifle\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
